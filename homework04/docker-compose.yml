services:
  # ollama server produces embedding vector and then generates answers based on rss content 
  # container loads 2 models 700MB and 4.7GB it takes some time
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    env_file:
      - .env
    ports:
      - "${LLM_PORT}:${LLM_PORT}"
    volumes:
      - ${LLM_HOST_MODEL_PATH}:/root/.ollama
    post_start:
      - command: ['sh', '-c', 'ollama pull ${LLM_EMBEDDING_MODEL} && ollama pull ${LLM_GENERATIVE_MODEL}']
        user: root
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all 
              capabilities: [gpu]

  # database, contains rss news + embedding vectors
  database:
    image: ankane/pgvector:latest
    container_name: postgres
    env_file:
      - .env
    ports:
      - "${POSTGRES_PORT}:${POSTGRES_PORT}"
    volumes:
      - ${POSTGRES_HOST_DB_PATH}:/var/lib/postgresql/data
    healthcheck:
      test: [ "CMD-SHELL", "sh -c 'pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB}'" ]
      interval: 5s
      timeout: 1s
      retries: 5     

  # database migration
  migrations:
    image: ghcr.io/kukymbr/goose-docker:3.19.2
    restart: "no"
    network_mode: host
    env_file:
      - .env
    environment:
      - GOOSE_DRIVER=postgres
      - GOOSE_DBSTRING=host=${POSTGRES_HOST}  port=${POSTGRES_PORT} user=${POSTGRES_USER} dbname=${POSTGRES_DB} password=${POSTGRES_PASSWORD} sslmode=disable
      - GOOSE_VERBOSE=true
    depends_on: 
      database:
        condition: service_healthy
    volumes:
      - ./migrations:/migrations

  # REST API server
  api-service:
    build: .
    container_name: api-service
    restart: "no"
    network_mode: host
    ports:
      - "${REST_API_PORT}:${REST_API_PORT}"
    command: |
      ./api-service \
      --db="postgres://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${POSTGRES_HOST}:${POSTGRES_PORT}/${POSTGRES_DB}" \
      --ollama="http://${LLM_HOST}:${LLM_PORT}" \
      --emb="${LLM_EMBEDDING_MODEL}" \
      --gen="${LLM_GENERATIVE_MODEL}" \
      --port=${REST_API_PORT}

    depends_on: 
      database:
        condition: service_healthy

  # daemon which checking for new articles from channels
  # and the push them to database
  channel-service:
    build: .
    container_name: channel-service
    restart: "no"
    network_mode: host
    command: |
      ./channel-service \
      --db="postgres://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${POSTGRES_HOST}:${POSTGRES_PORT}/${POSTGRES_DB}"
    depends_on: 
      database:
        condition: service_healthy

  # daemon which responsible for downloading articles, 
  # extract text and generate context embedding vectors  
  news-service:
    build: .
    container_name: news-service
    restart: "no"
    network_mode: host
    command: |
      ./news-service \
      --db="postgres://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${POSTGRES_HOST}:${POSTGRES_PORT}/${POSTGRES_DB}" \
      --ollama="http://${LLM_HOST}:${LLM_PORT}" \
      --emb="${LLM_EMBEDDING_MODEL}" \
      --gen="${LLM_GENERATIVE_MODEL}"
    depends_on: 
      database:
        condition: service_healthy

# for testing purpose only, should be removed
# Allows to look at and remove the database and models without additional permissions
  fix_permissions:
    image: alpine
    restart: "no"
    command: ["sh", "-c", "chmod -R 777 /var/lib/postgresql/data && chmod -R 777 /root/.ollama"]
    volumes:
      - ${POSTGRES_HOST_DB_PATH}:/var/lib/postgresql/data
      - ${LLM_HOST_MODEL_PATH}:/root/.ollama
    depends_on: 
      database:
        condition: service_healthy